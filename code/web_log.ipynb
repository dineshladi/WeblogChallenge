{"cells":[{"cell_type":"code","source":["### Read Log Files to an RDD\n#dataPath = \"/FileStore/tables/sample_log.txt\"\ndataPath = \"/FileStore/tables/2015_07_22_mktplace_shop_web_log_sample_log-214a9.gz\"\nraw_data = sc.textFile(dataPath).map(lambda row: row.split(\" \"))\nraw_data.take(1)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Functions to clean timestamp, IP and request type\nfrom dateutil.parser import parse\ndef parseDateTime(data):\n  new_data = parse(data.replace(\"T\",\" \").split(\".\")[0])\n  #new_data = data.replace(\"T\",\" \").split(\".\")[0]\n  return new_data\n\ndef parseIP(ip):\n  new_ip = ip.split(\":\")[0]\n  return new_ip\n\ndef parseReq(request):\n  return request.replace('\"','')\n\ndef parseURL(raw_url):\n  return raw_url.split(\"?\")[0]"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# filter RDD to use only IP_ADDRESS, Timestamp, URL and request type columns \ntime_ip_RDD = raw_data.map(lambda r: (parseIP(r[2]),parseDateTime(r[0]),parseReq(r[11]),parseURL(r[12])))\ntime_ip_RDD.take(10)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Defining schema of the dataframe and converting RDD to dataframe using the schema\nfrom pyspark.sql.types import *\nfields = [StructField(\"IP_ADDRESS\", StringType(), True),StructField(\"TIME_STAMP\",TimestampType(), True),StructField(\"REQUEST_TYPE\", StringType(),True),StructField(\"URL\", StringType(), True)]\nschema = StructType(fields)\n\ndf = spark.createDataFrame(time_ip_RDD, schema)\ndf.head(5)\ndf.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", True).save(\"/FileStore/weblog/raw_logs\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["##### Main Processing steps \n#### 1) Find difference between subsequent timestamps after ordering for an given IP\n#### 2) Apply threshold filter and checkpoint start and end of an session and group the records to a session\n#### 3) Flag records to GET type request or POST type request \nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nthreshold = 900   ### Inactivity threshold interval 5 min\nordered_df = df.withColumn(\"PREV_TIME_STAMP\",F.lag(\"TIME_STAMP\",1).over(Window().partitionBy(\"IP_ADDRESS\").orderBy(\"TIME_STAMP\")))\nordered_df = ordered_df.withColumn(\"TIME_DIFF\", F.unix_timestamp(\"TIME_STAMP\") - F.unix_timestamp(\"PREV_TIME_STAMP\"))\nordered_df = ordered_df.withColumn(\"Session_Ind\",F.when((ordered_df.TIME_DIFF > threshold) | (F.isnull(ordered_df.TIME_DIFF) ) , 1).otherwise(0))\nordered_df = ordered_df.withColumn(\"Session_Group\",F.sum(ordered_df.Session_Ind).over(Window().partitionBy(\"IP_ADDRESS\").orderBy(\"TIME_STAMP\")))\n#ordered_df = ordered_df.withColumn(\"Start_Time\",F.min(ordered_df.TIME_STAMP).over(Window().partitionBy(\"IP_ADDRESS\",\"Session_Group\")))\n#ordered_df = ordered_df.withColumn(\"End_Time\",F.max(ordered_df.TIME_STAMP).over(Window().partitionBy(\"IP_ADDRESS\",\"Session_Group\")))\nordered_df = ordered_df.withColumn(\"Get_Ind\",F.when(ordered_df.REQUEST_TYPE==\"GET\",1).otherwise(0))\nordered_df = ordered_df.withColumn(\"Post_Ind\",F.when(ordered_df.REQUEST_TYPE==\"POST\",1).otherwise(0))\nordered_df.show(200)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["ordered_df.groupBy(ordered_df.REQUEST_TYPE).count().show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["### Create Master DataSet on User level and session level \n### Get start time, end time, url count, distinct url count, session time and starting point url\n### I think the above attributes are the most important in profiling data on user x session level \nordered_df.createOrReplaceTempView(\"logs\")\nmaster_df = spark.sql(\"SELECT tbl.*,(unix_timestamp(tbl.End_Time)-unix_timestamp(tbl.Start_Time)) AS Session_Time, tbl2.URL as Start_URL from (SELECT IP_ADDRESS, Session_Group, min(TIME_STAMP) as Start_Time, max(Time_Stamp) as End_Time,sum(Get_Ind) as Get_Count, sum(Post_Ind) as Post_Count, count(URL) as URL_Count ,count(distinct URL) as Unique_URL_Count from logs group by 1,2)tbl LEFT JOIN (select IP_ADDRESS,URL,Session_Group from logs where TIME_DIFF is null or TIME_DIFF > 900 )tbl2 USING (IP_ADDRESS,Session_Group)\")\n#master_df = spark.sql(\"SELECT IP_ADDRESS, Session_Group, min(TIME_STAMP) as Start_Time, max(Time_Stamp) as End_Time, sum(Get_Ind) as Get_Count, sum(Post_Ind) as Post_Count,count(URL) as URL_Count, count(distinct URL) as Unique_URL_Count from logs group by 1,2\")\nmaster_df.orderBy(\"IP_ADDRESS\",\"Session_Group\").show(200)\nmaster_df.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\",True).save(\"/FileStore/weblog/master_logs\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["### Average Session Time \nmaster_df.createOrReplaceTempView(\"master_logs\")\nresults = spark.sql(\"SELECT avg(Session_Time) FROM master_logs\")\nresults.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#### Unique URL Hits per session\nuniq_hits = spark.sql(\"select IP_ADDRESS,Session_Group,Unique_URL_Count from master_logs order by IP_ADDRESS\")\nuniq_hits.show(200)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["uniq_hits.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", True).save(\"/FileStore/weblog/unique_hits\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["### Most Engaged Users Session Wise\nengaged_users = spark.sql(\"select IP_ADDRESS,Session_Group,Session_Time from master_logs ORDER BY Session_Time DESC\")\nengaged_users.show(50)\n                          "],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["engaged_users.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", True).save(\"/FileStore/weblog/engaged_users\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["### Most Engaged Users based on sum of total session times \ntotal_engaged_users = spark.sql(\"SELECT IP_ADDRESS, SUM(Session_Time) AS Total_Session_Time FROM master_logs GROUP BY IP_ADDRESS ORDER BY Total_Session_Time DESC\")\ntotal_engaged_users.show(50)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#df.createOrReplaceTempView(\"raw_logs\")\n#spark.sql(\"SELECT max(TIME_STAMP),min(TIME_STAMP) from raw_logs\").show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["## Distribution of session times at different times of the day to check if there is an instrisic pattern of session times at different hours of the day\nspark.sql(\"SELECT Start_Time, Session_Time from master_logs\").toPandas().set_index('Start_Time').resample('30min').mean().plot(kind='bar',figsize=(30,20))\ndisplay()\n### There is no intrinsic distrubution on average session times at different hours of the day. I think data for more number of days would help out in figuring out a pattern"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["\n#### Predicting the load on the server: Ideally if there is a pattern in the time series data, then we apply ARIMA model to fit the data. But in the one day's data, there is no prper pattern\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["## Creating time series data on min level\nimport pandas as pd \n\nts_df = df.select(\"TIME_STAMP\",\"URL\").toPandas()\ndf.select(\"TIME_STAMP\",\"URL\").repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\",True).save(\"/FileStore/weblog/timeseries_df\")\nts_df = ts_df.set_index('TIME_STAMP').resample('1min').count()\nts_df = ts_df[\"URL\"]/60   ### requests per second "],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["ts_df.sort_values(ascending=False).head(100)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["spark.sql(\"select count(*),count(distinct Start_URL) from master_logs\").show()\n### Lot of distinct starting urls compared to total number of rows in master dataset"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#dbutils.fs.rm(\"/FileStore/weblog/\",True)\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"web_log","notebookId":382118239525232},"nbformat":4,"nbformat_minor":0}
